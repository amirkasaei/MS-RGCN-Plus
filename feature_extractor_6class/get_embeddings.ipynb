{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTApeE4tFuJO"
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "h3Y-OP0ZJFke"
   },
   "outputs": [],
   "source": [
    "# --- config ---\n",
    "\n",
    "num_classes = 6\n",
    "batch_size = 8\n",
    "magnifications = magnifications = [10, 20, 40]   #TODO  # we'll filter the CSV by these later\n",
    "patch_size = 512\n",
    "folds = ['fold1']\n",
    "\n",
    "# device to run on\n",
    "device = 'cuda'\n",
    "\n",
    "# CSV with absolute patch paths + labels (hard_label, p0..p5)\n",
    "labels_csv = \"../data/VPC/patch_labels_majority.csv\"  # <-- set to your CSV path\n",
    "\n",
    "# where your trained model checkpoints live\n",
    "path_model = '../models/model_VPC/'           # base dir that contains per-magnification/fold folders, if you use that layout\n",
    "model_name = '256_aug_model'               # used only if you auto-build paths later\n",
    "\n",
    "# OPTION 1 (explicit): set the exact checkpoint file you want to use\n",
    "checkpoint_file = \"../models/model_VPC/40/fold1/256_aug_model_039_0.7346.pt\" # e.g., \"model_VPC_Zurich/40/fold1/256_aug_model_030_0.8123.pt\"\n",
    "\n",
    "# OPTION 2 (pattern): if you prefer to pick the latest/best later, keep None here and we’ll select programmatically\n",
    "\n",
    "# where to save extracted embeddings\n",
    "path_embeddings = '256_VPC_embeddings/'\n",
    "import os\n",
    "os.makedirs(path_embeddings, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUEUvDLIbLnw"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qQb9tJzebNW_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11179590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/miniconda3/envs/msrgcn/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user01/miniconda3/envs/msrgcn/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "#         self.num_classes = num_classes\n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.model.fc = nn.Sequential(nn.Linear(in_features=512, out_features=num_classes, bias=True, ))#,\n",
    "\n",
    "\n",
    "    def forward(self, dictionary):\n",
    "        return {'label': self.model(dictionary['img'])}\n",
    "\n",
    "    def prediction(self, dictionary):\n",
    "        return {'label': torch.argmax(self.forward(dictionary)['label'], dim=1)}\n",
    "\n",
    "model = NN(num_classes=num_classes).cuda()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH-6cE6oHyQD"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "h-w_nD4xGfGB"
   },
   "outputs": [],
   "source": [
    "# a function to move tensors from the CPU to the GPU\n",
    "def dict_to_device(orig, device):\n",
    "    new = {}\n",
    "    for k,v in orig.items():\n",
    "        new[k] = v.to(device)\n",
    "    return new\n",
    "\n",
    "def plotImage(img, ax=plt):\n",
    "    img_pil = torchvision.transforms.ToPILImage()(img)\n",
    "    img_size = torch.FloatTensor(img_pil.size)\n",
    "    ax.imshow(img_pil)\n",
    "\n",
    "def directory_maker(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def listdir_fullpath(d):\n",
    "    return [os.path.join(d, f) for f in os.listdir(d)]\n",
    "        \n",
    "def path_constructor(root_dir, embd_dir, magnifications, sizes):\n",
    "    dict_imgs_path = {mag: {core : [] for core in os.listdir(root_dir)} for mag in magnifications}\n",
    "    for core in os.listdir(root_dir):\n",
    "        embd_core_path = embd_dir + core + '/'\n",
    "        directory_maker(embd_core_path)\n",
    "        for size in sizes:\n",
    "            embd_size_path = embd_core_path + str(size) + '/'\n",
    "            directory_maker(embd_size_path)\n",
    "            for mag in magnifications:\n",
    "                embd_mag_path = embd_size_path + str(mag) + '/'\n",
    "                directory_maker(embd_mag_path)\n",
    "                dict_imgs_path[mag][core].extend(listdir_fullpath(root_dir + core + '/' + str(size) + '/' + str(mag) + '/'))\n",
    "    return dict_imgs_path\n",
    "    \n",
    "# dict_imgs = path_constructor(path_VPC, path_embeddings + fold + '/', [10, 20, 40], [patch_size])\n",
    "\n",
    "## get embeddings\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output\n",
    "    return hook\n",
    "\n",
    "def load_model(checkpoint_file, embedding_layer):\n",
    "    \"\"\"\n",
    "    Loads weights into the already-instantiated `model` variable, registers a hook\n",
    "    on the penultimate layer (avgpool for ResNet-18), and sets eval mode.\n",
    "    \"\"\"\n",
    "    ckpt = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model.to(device)\n",
    "    # hook at penultimate layer: AdaptiveAvgPool2d output [B, 512, 1, 1]\n",
    "    model.model.avgpool.register_forward_hook(get_activation(embedding_layer))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# load_model(path_model, 'aug_model', magnification, fold)\n",
    "\n",
    "def save_embeddings(model, imgs_path, embd_dir, embedding_layer, model_name):\n",
    "    target_path = embd_dir + '/'.join(imgs_path[0].split('/')[-4:-1]) + '/' + model_name + '_' + embedding_layer + '.pkl'\n",
    "    if os.path.exists(target_path): \n",
    "        return\n",
    "\n",
    "    # NEW: make sure nested folders exist\n",
    "    os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "\n",
    "    transform = transforms.ToTensor()  # keep as training (no Normalize if you didn't use it in training)\n",
    "    dict_embd = {img_path.split('/')[-1][:-4] : None for img_path in imgs_path}\n",
    "\n",
    "    for img_path in imgs_path:\n",
    "        if not img_path.endswith('.png'):\n",
    "            print(img_path)\n",
    "            continue\n",
    "        img = io.imread(img_path)\n",
    "        if img.ndim == 3 and img.shape[2] == 4:\n",
    "            img = img[:, :, :3]\n",
    "        img = cv.resize(img, (256, 256), interpolation=cv.INTER_CUBIC)  # keep 256 to match training\n",
    "        img = transform(img)\n",
    "\n",
    "        dict_gpu = dict_to_device({'img': torch.unsqueeze(img, 0)}, device)\n",
    "        with torch.no_grad():\n",
    "            model(dict_gpu)\n",
    "        # avgpool output is [1, 512, 1, 1] -> squeeze to [512]\n",
    "        dict_embd[img_path.split('/')[-1][:-4]] = torch.squeeze(activation[embedding_layer]).cpu().detach().numpy()\n",
    "\n",
    "    with open(target_path, 'wb') as f:\n",
    "        pickle.dump(dict_embd, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_CRmJntGgwi"
   },
   "source": [
    "# Embeddings saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1LzS0KGzGiqg"
   },
   "outputs": [],
   "source": [
    "def get_embeddings_all(embd_dir, model_name, magnifications, folds,\n",
    "                       embedding_layer='avgpool', sizes=[512]):\n",
    "    \"\"\"\n",
    "    CSV-driven embedding extraction.\n",
    "    - Reads absolute image paths from `labels_csv`\n",
    "    - Filters by `magnifications`\n",
    "    - Groups by core (e.g., slide001_core003)\n",
    "    - Loads a single checkpoint from `checkpoint_file`\n",
    "    - Saves one pickle per (core/size/mag), same layout your save_embeddings expects\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # 1) Build dict_imgs_path[magnification][core] = [list of image paths] from the CSV\n",
    "    df = pd.read_csv(labels_csv)\n",
    "\n",
    "    # Filter by magnifications (assumes path like .../<size>/<mag>/<file>.png)\n",
    "    if magnifications:\n",
    "        pat = \"|\".join([f\"/{m}/\" for m in magnifications])\n",
    "        df = df[df[\"path\"].str.contains(pat)]\n",
    "\n",
    "    dict_imgs_path = {mag: {} for mag in magnifications}\n",
    "    for p in df[\"path\"].tolist():\n",
    "        parts = p.split(\"/\")\n",
    "        # Expected tail: [..., slide001_core003, 512, 40, filename.png]\n",
    "        core = parts[-4]              # slide001_core003\n",
    "        mag = int(parts[-2])          # 10/20/40\n",
    "        dict_imgs_path.setdefault(mag, {}).setdefault(core, []).append(p)\n",
    "\n",
    "    print(\"images loaded from CSV\")\n",
    "\n",
    "    # 2) Load the trained model ONCE from your explicit checkpoint and register hook\n",
    "    model = load_model(checkpoint_file, embedding_layer)\n",
    "    print(f\"model loaded from {checkpoint_file}\")\n",
    "    model.eval()\n",
    "\n",
    "    # 3) Iterate groups and dump embeddings (per fold just controls output folder prefix)\n",
    "    for fold in folds:\n",
    "        cnt = 0\n",
    "        for magnification in magnifications:\n",
    "            for core, paths in dict_imgs_path.get(magnification, {}).items():\n",
    "                save_embeddings(model, paths, embd_dir + fold + \"/\", embedding_layer, model_name)\n",
    "                cnt += 1\n",
    "                if cnt % 100 == 0:\n",
    "                    print(f\"{fold}, magnification {magnification}: {cnt} cores processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images loaded from CSV\n",
      "model loaded from ../models/model_VPC/40/fold1/256_aug_model_039_0.7346.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_381899/4007330687.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_file, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold1, magnification 10: 100 cores processed\n",
      "fold1, magnification 10: 200 cores processed\n",
      "fold1, magnification 20: 300 cores processed\n",
      "fold1, magnification 20: 400 cores processed\n",
      "fold1, magnification 40: 500 cores processed\n",
      "fold1, magnification 40: 600 cores processed\n",
      "fold1, magnification 40: 700 cores processed\n"
     ]
    }
   ],
   "source": [
    "get_embeddings_all(path_embeddings, model_name, magnifications, folds, sizes=[patch_size])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "msrgcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b4c29892f0c4c0e9b7176b867aca14c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2969031089f34cbdbe70c405167673c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40b3dbde3e894eb78f45e9d2342aa714": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc24c5ac94e24a8b8d0e10d07142bf48",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e26289495af6433dbf3d4c3f7c134945",
      "value": 46830571
     }
    },
    "5a1bbf7b9c904cb295cb486bddacd2f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b13f258874e4d89bdf5640e70cdd3a4",
      "placeholder": "​",
      "style": "IPY_MODEL_ee13ea848e1c48f4958890da00770737",
      "value": "100%"
     }
    },
    "8b13f258874e4d89bdf5640e70cdd3a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5b4b114bade4395952a717e239c6fcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a1bbf7b9c904cb295cb486bddacd2f0",
       "IPY_MODEL_40b3dbde3e894eb78f45e9d2342aa714",
       "IPY_MODEL_d62ae91d208241eeb980c7b683cdff1b"
      ],
      "layout": "IPY_MODEL_2969031089f34cbdbe70c405167673c2"
     }
    },
    "cc24c5ac94e24a8b8d0e10d07142bf48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d62ae91d208241eeb980c7b683cdff1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecd2309de7dd41d290f9ed71e085e4d4",
      "placeholder": "​",
      "style": "IPY_MODEL_0b4c29892f0c4c0e9b7176b867aca14c",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 180MB/s]"
     }
    },
    "e26289495af6433dbf3d4c3f7c134945": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ecd2309de7dd41d290f9ed71e085e4d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee13ea848e1c48f4958890da00770737": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
